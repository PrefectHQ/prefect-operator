apiVersion: prefect.io/v1
kind: PrefectDeployment
metadata:
  name: prefectdeployment-selfhosted-sample
spec:
  # Self-hosted Prefect server configuration
  server:
    remoteApiUrl: "https://prefect-server.example.com/api"
    apiKey:
      valueFrom:
        secretKeyRef:
          name: prefect-server-api-key
          key: api-key

  # Work pool configuration - reference to a namespaced work pool
  workPool:
    namespace: "prefect-system"
    name: "kubernetes-work-pool"
    workQueue: "high-priority"

  # Deployment configuration
  deployment:
    description: "Sample ML training deployment on self-hosted Prefect"
    tags:
      - "ml"
      - "training"
      - "development"
    labels:
      environment: "development"
      team: "ml-platform"
      model: "sentiment-analysis"

    entrypoint: "ml/training_flow.py:train_model"
    path: "/opt/prefect/ml"

    # Version information
    versionInfo:
      type: "git"
      version: "v2.1.0"

    # Pull steps for code retrieval
    pullSteps:
      - prefect.deployments.steps.git_clone:
          repository: "https://github.com/example/ml-flows.git"
          branch: "main"
      - prefect.deployments.steps.pip_install_requirements:
          requirements_file: "requirements.txt"

    # Parameter schema enforcement
    parameterOpenApiSchema:
      type: "object"
      properties:
        model_name:
          type: "string"
          description: "Name of the model to train"
        epochs:
          type: "integer"
          minimum: 1
          maximum: 100
          default: 10
        learning_rate:
          type: "number"
          minimum: 0.0001
          maximum: 1.0
          default: 0.001
      required:
        - "model_name"

    enforceParameterSchema: true

    # Default parameters
    parameters:
      model_name: "sentiment-classifier"
      epochs: 20
      learning_rate: 0.001
      batch_size: 32

    # Infrastructure configuration
    jobVariables:
      image: "ml-registry/training:latest"
      cpu: "2"
      memory: "4Gi"
      gpu: "1"
      nodeSelector:
        gpu: "nvidia-tesla-v100"
      env:
        - name: "CUDA_VISIBLE_DEVICES"
          value: "0"
        - name: "PYTORCH_CUDA_ALLOC_CONF"
          value: "max_split_size_mb:512"

    # No schedule - runs on-demand only
    paused: false
